{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# X‑ray Fracture Detection — Colab Template\n*Auto-generated on 2025-09-01. Plug in your dataset path in Drive and run top-to-bottom.*\n\nThis notebook is structured to support **incremental experiments**:\n1) Configuration  2) Environment setup  3) Data loading  4) Training  5) Evaluation  6) LIME explanations  7) Save artifacts.\n\n> Tip: Keep data in Google Drive, code in GitHub. Use a Colab badge in your README."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 0. Configuration"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n# === User-configurable settings ===\nPROJECT_NAME = \"xray-fracture-experiments\"\nDATA_DIR = \"/content/drive/MyDrive/fracatlas/data\"  # change this to your dataset path\nEXP_DIR = \"/content/drive/MyDrive/fracatlas/experiments\"  # where to save runs/models\nMODEL_NAME = \"resnet50\"\nIMG_SIZE = 224\nBATCH_SIZE = 32\nNUM_EPOCHS = 5            # increase when ready\nLR = 1e-4\nSEED = 42\n\n# Optional: If you host code in GitHub, set these and uncomment clone step below\nGITHUB_USER = \"\"     # e.g., \"your-username\"\nGITHUB_REPO = \"\"     # e.g., \"fracture-detection\"\nGITHUB_BRANCH = \"main\"\n\nprint(\"Config loaded.\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Environment & Drive"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n# If on Colab, install/upgrade deps\ntry:\n    import google.colab  # type: ignore\n    IN_COLAB = True\nexcept Exception:\n    IN_COLAB = False\n\nif IN_COLAB:\n    !pip -q install torch torchvision lime matplotlib --upgrade\n\nfrom google.colab import drive as _drive if IN_COLAB else None\nif IN_COLAB:\n    _drive.mount('/content/drive')\n\nimport os, random, numpy as np, torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader\n\n# Reproducibility\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\nrandom.seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### (Optional) Clone your GitHub repo into the runtime"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n# Uncomment to clone your repo; notebooks/scripts will be available under /content/<repo>\n# if GITHUB_USER and GITHUB_REPO:\n#     !git clone -b {GITHUB_BRANCH} https://github.com/{GITHUB_USER}/{GITHUB_REPO}.git\n#     %cd {GITHUB_REPO}\n# else:\n#     print(\"Skipping GitHub clone (no repo configured).\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Data Loading (ImageFolder layout)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\n# Expected structure:\n# DATA_DIR/\n#   train/\n#     fracture/\n#     non-fracture/\n#   val/\n#     fracture/\n#     non-fracture/\n\nfrom torchvision import transforms\ntransform = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],\n                         [0.229, 0.224, 0.225])\n])\n\ntrain_dataset = datasets.ImageFolder(os.path.join(DATA_DIR, 'train'), transform=transform)\nval_dataset   = datasets.ImageFolder(os.path.join(DATA_DIR, 'val'), transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\nval_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n\nclass_names = train_dataset.classes\nprint(\"Classes:\", class_names)\nprint(f\"Train: {len(train_dataset)} images | Val: {len(val_dataset)} images\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Model (ResNet50 baseline)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\nmodel = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, 2)  # binary classification\nmodel = model.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=LR)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Training Loop"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\nimport math, time\n\ndef train_one_epoch(loader):\n    model.train()\n    total_loss, total_correct, total = 0.0, 0, 0\n    for x, y in loader:\n        x, y = x.to(device), y.to(device)\n        optimizer.zero_grad()\n        out = model(x)\n        loss = criterion(out, y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * x.size(0)\n        total_correct += (out.argmax(1) == y).sum().item()\n        total += x.size(0)\n    return total_loss/total, total_correct/total\n\n@torch.no_grad()\ndef evaluate(loader):\n    model.eval()\n    total_loss, total_correct, total = 0.0, 0, 0\n    for x, y in loader:\n        x, y = x.to(device), y.to(device)\n        out = model(x)\n        loss = criterion(out, y)\n        total_loss += loss.item() * x.size(0)\n        total_correct += (out.argmax(1) == y).sum().item()\n        total += x.size(0)\n    return total_loss/total, total_correct/total\n\nhist = []\nfor epoch in range(NUM_EPOCHS):\n    tr_loss, tr_acc = train_one_epoch(train_loader)\n    val_loss, val_acc = evaluate(val_loader)\n    hist.append((epoch+1, tr_loss, tr_acc, val_loss, val_acc))\n    print(f\"Epoch {epoch+1:02d} | train loss {tr_loss:.4f} acc {tr_acc:.3f} | val loss {val_loss:.4f} acc {val_acc:.3f}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. LIME Explanations (single image demo)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\nfrom lime import lime_image\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage.segmentation import mark_boundaries\n\n# Helper to run model on a numpy image batch (HxWxC, uint8)\ndef batch_predict(images):\n    model.eval()\n    batch = []\n    for i in images:\n        img = Image.fromarray(i)\n        t = transforms.Compose([\n            transforms.Resize((IMG_SIZE, IMG_SIZE)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n        ])(img).to(device)\n        batch.append(t)\n    batch = torch.stack(batch, dim=0)\n    logits = model(batch)\n    probs = torch.softmax(logits, dim=1).detach().cpu().numpy()\n    return probs\n\n# Pick one sample from validation set\nimg_path, true_label = val_dataset.samples[0]\nraw = Image.open(img_path).convert(\"RGB\")\nexplainer = lime_image.LimeImageExplainer()\nexplanation = explainer.explain_instance(\n    np.array(raw),\n    batch_predict,\n    top_labels=2,\n    hide_color=0,\n    num_samples=1000\n)\ntop = explanation.top_labels[0]\ntemp, mask = explanation.get_image_and_mask(\n    label=top,\n    positive_only=True,\n    num_features=5,\n    hide_rest=False\n)\nplt.figure()\nplt.imshow(mark_boundaries(temp/255.0, mask))\nplt.title(f\"LIME (pred label={top}, true={true_label})\")\nplt.axis(\"off\")\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. Save Artifacts (model + history)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "\nos.makedirs(EXP_DIR, exist_ok=True)\nmodel_path = os.path.join(EXP_DIR, f\"{PROJECT_NAME}_{MODEL_NAME}.pt\")\ntorch.save(model.state_dict(), model_path)\n\n# Save simple history CSV\nimport csv\nhist_path = os.path.join(EXP_DIR, f\"{PROJECT_NAME}_history.csv\")\nwith open(hist_path, \"w\", newline=\"\") as f:\n    w = csv.writer(f)\n    w.writerow([\"epoch\",\"train_loss\",\"train_acc\",\"val_loss\",\"val_acc\"])\n    for row in hist:\n        w.writerow(row)\n\nprint(\"Saved:\", model_path)\nprint(\"Saved:\", hist_path)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7. Next Steps"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "- Try **EfficientNet** or **Swin Transformer** backbones\n- Handle class imbalance with **Weighted CE** or **Focal Loss**\n- Add proper metrics (AUC, sensitivity, specificity)\n- If you have boxes/masks, train **YOLO/Faster R-CNN** or **UNet** for localization\n- Log experiments with **TensorBoard** or **Weights & Biases**"
    }
  ]
}